{"chunk_id": "0b73bb2e-d531-4899-bdc8-a0a9c2661ab0", "text": "Aniket Kulkarni <anikerry@gmail.com>Thesis Regulations at Bosch3 messagesKshirasagar Shreya (CR/ASD4) <Shreya.Kshirasagar@de.bosch.com>Wed, 26 Mar at 18:27To: aniketdattatraya.kulkarni@study.thws.de <aniketdattatraya.kulkarni@study.thws.de>, magda.gregorova@thws.de<magda.gregorova@thws.de>Cc: anikerry@gmail.com <anikerry@gmail.com>Hello all, I discussed internally regarding the requirements for master’s thesis we talked about in our meeting thisafternoon. Please find the details below: This master’s thesis is funded by an EU funded project. According to the consortium agreements, codesharing is only permitted with the involved parties, which unfortunately does not apply to THWS.Additionally, as per Bosch rules (also included in the contract), it is highly unlikely we can open accessthe code. In case of consortium rules, there might be a possibility to open-access code only after 3-5years following the end of the consortium contract i.e., around 2029 or 2031. Bosch however has stricterrules regarding this and as I understand, the code would belong to Bosch. Regarding possibility of journal publication, it typically requires at least a year’s work to achieve qualityresults. In case of a master thesis, generally, we don’t see that quality of results that could technicallyfruition to journal articles within 6 months. A workshop paper I imagine could be feasible, depending onthe outcome of the thesis and approval from the higher body within our department. I believe it is crucial to be transparent from the beginning to avoid any misunderstandings regardingexpectations and outcomes. Taking these things into consideration please let me know if these requirementsare still feasible or if we need to rethink the contract. Thank you for your attention to this matter. Mit freundlichen Grüßen / Best regardsShreya KshirasagarRobert Bosch GmbH | Renningen | 70465 Stuttgart | GERMANY | www.bosch.comshreya.kshirasagar@de.bosch.comGregorová, Magda <magda.gregorova@thws.de>Wed, 26 Mar at 21:03", "source": "Thesis Regulations at Bosch.pdf", "page": 1}
{"chunk_id": "a3819df5-2c5e-4e4f-a48f-a2eb21760388", "text": "To: Kulkarni, Aniket <aniketdattatraya.kulkarni@study.thws.de>, Shreya.Kshirasagar@de.bosch.com<Shreya.Kshirasagar@de.bosch.com>Cc: anikerry@gmail.com <anikerry@gmail.com>Hi all, thanks for checking this so quickly. You rightly understood that this may be a critical point. Unfortunately, companystandard protecting their own company interests often go against academic rules protecting the interests of thestudents. As for publication, I totally agree that it is very difficult if not outright impossible to reach within 6 monthssufficiently good results so that the outcomes can be published within a journal. And I totally understand and agreethat if the work were to be turned into a journal publication, more people from the project (I guess primarily fromBosch but perhaps also from Eidhoven) need to be involved. And this could easily take 12 months or more. Whetherthe outcomes could be good enough for a workshop is also questionable but even then, Bosch and Eidhovencollaborators would need to get involved. Both of these scenarios are, however, fairly irrelevant for the Master thesisitself and we do not really need to discuss them at this point. They will only become relevant if the outcomes aretruly excellent and one of the involved parties is interested in pursuing the journal/workshop publication.So on this point, I think we totally agree and journal/workshop publication is by no means acondition for Aniket's master thesisAniket cannot, however, finish his Master's program simply through an internship for Boschwithout having anything in his hands in the end. By the rules of the THWS, Aniket has to deliver aMaster thesis which has to be a result of his independent scientific work.Nevertheless, since the Master thesis is still part of Aniket's educational process, he obviouslydoes not have to work on it completely alone but it is expect he delivers it under a supervision. Ifthis supervision comes from a company due to Aniket's involvement in a bigger project, evenbetter. But Aniket still needs to finish with a document in the end which has been fully developedby him as the single author taking full responsibility for its content. Strictly speaking this meansthat neither I nor you can \"tell\" Aniket what to write. We can \"advice\" him, but it is his decision ifand how he does it.In case of company driven project, the rules are similar with the fundamental difference that thecompany can tell Aniket what NOT to write about and what CANNOT be included in themanuscript because it relates to sensitive company (or partner) information. For example, thecode could be an example of such a restriction (though I personally still very much disagree for aMaster thesis in AI but can live with). If necessary, the company can also check the manuscriptbefore submission. But Aniket has to be able to have in the end a document that he can submit toTHWS as a certification of his work and use in the future as an example of his own scientific work,e.g. when searching for future jobs. This is an absolutely critical point but I hope this has been clear from the beginning - it is inthe end a Master thesis.An important point related to the above: as I have explained, we have now relaxed the rules for themanuscript itself from the hefty 80+ to  a lighter cca 20page jurnal-like format.", "source": "Thesis Regulations at Bosch.pdf", "page": 2}
{"chunk_id": "358a3f76-1974-4279-a3de-d067e89171fc", "text": "If necessary, the company can also check the manuscriptbefore submission. But Aniket has to be able to have in the end a document that he can submit toTHWS as a certification of his work and use in the future as an example of his own scientific work,e.g. when searching for future jobs. This is an absolutely critical point but I hope this has been clear from the beginning - it is inthe end a Master thesis.An important point related to the above: as I have explained, we have now relaxed the rules for themanuscript itself from the hefty 80+ to  a lighter cca 20page jurnal-like format. In principle, thisshould make it easier for the company as well (shorter document to review). We, however, stillhave to be able to evaluate Aniket's work in terms of scientific contribution. This means that this20pages have to be very content heavy. If the code has to be left out, I am getting a little worriedhere. If there were even more restrictions on the content from the company side, e.g. leaving outanalytical results etc., I am afraid we would have to go back to the usual 80+ pages tocomplement the missing practical parts (due to being blocked by the company) with moretheoretical discussion and analysis and perhaps even experiments unrelated to the work in Bosch.For example, spiking neural networks are a topic on its own without any proprietary rights of theBosch consortia. If necessary, I could imagine Aniket working on this independent part of researchafter finishing his internship in Bosch and hence delivering the thesis quite a bit later. I would haveno problem with such a setup if it seemed easier for everybody involved. This is very much forAniket to consider and perhaps discuss with you.", "source": "Thesis Regulations at Bosch.pdf", "page": 2}
{"chunk_id": "4b287be9-fc40-457b-a480-050e53cf0548", "text": "If anything is not quite clear or still needs to be discussed, please feel free to bring it up. It is really important toavoid any misunderstandings on these points early on. Quite honestly, I am much less worried about the work itself than these legal proprietary stuff. It pains me that thereis still a great misalignment between company and academic interests complicating the opportunities for ourstudents.And yes, I do realize that you are just the poor PhD in the middle. :(Best[Quoted text hidden]Gregorová, Magda <magda.gregorova@thws.de>Wed, 26 Mar at 21:37To: Kulkarni, Aniket <aniketdattatraya.kulkarni@study.thws.de>, Shreya.Kshirasagar@de.bosch.com<Shreya.Kshirasagar@de.bosch.com>Cc: anikerry@gmail.com <anikerry@gmail.com>Hi all, on a more scientific and hence interesting note: I have remembered that there is a regular workshop on the topic atthe biggest European conference: https://sites.google.com/view/dlnh-ecmlpkdd24Perhaps there might be some interesting material to read :)Best[Quoted text hidden]", "source": "Thesis Regulations at Bosch.pdf", "page": 3}
{"chunk_id": "2d99f109-da32-46e3-b3a5-9793ec43e73c", "text": "THOR - A Neuromorphic Processor with 7.29G\nTSOP2/mm2Js Energy-Throughput Efﬁciency\nMayank Senapati1, Manil Dev Gomony1, Sherif Eissa1, Charlotte Frenkel2, and Henk Corporaal1\n1Eindhoven University of Technology\n2Delft University of Technology\nAbstract —Neuromorphic computing using biologically inspired\nSpiking Neural Networks (SNNs) is a promising solution to meet\nEnergy-Throughput (ET) efﬁciency needed for edge comput-\ning devices. Neuromorphic hardware architectures that emulate\nSNNs in analog/mixed-signal domains have been proposed to\nachieve order-of-magnitude higher energy efﬁciency than all-\ndigital architectures, however at the expense of limited scal-\nability, susceptibility to noise, complex veriﬁcation, and poor\nﬂexibility. On the other hand, state-of-the-art digital neuromor-\nphic architectures focus either on achieving high energy efﬁ-\nciency (Joules/synaptic operation (SOP)) or throughput efﬁciency\n(SOPs/second/area), resulting in poor ET efﬁciency. In this work,\nwe present THOR, an all-digital neuromorphic processor with a\nnovel memory hierarchy and neuron update architecture that\naddresses both energy consumption and throughput bottlenecks.\nWe implemented THOR in 28nm FDSOI CMOS technology and\nour post-layout results demonstrate an ET efﬁciency of 7.29G\nTSOP2/mm2Js at 0.9V , 400 MHz, which represents a 3X im-\nprovement over state-of-the-art digital neuromorphic processors.\nI. I NTRODUCTION\nNeuromorphic computing using biologically inspired Spik-\ning Neural Networks (SNN) has arisen as a new paradigm that\ncan accommodate energy and throughput requirements of edge\nAI processing [1]. Neuromorphic hardware aims to emulate\nhuman brain operations and offers various advantages over\ntraditional systems, including sparse low-power computation\nand highly scalable parallel processing [2]. Energy efﬁciency\n(Joules/synaptic operation (SOP)) and throughput efﬁciency\n(SOPs/second/area) are the two key metrics to evaluate a Neu-\nromorphic architecture for edge AI applications. We combine\nthese two metrics into one single ﬁgure of merit called Energy-\nThroughput (ET) efﬁciency in terms of GSOP2/mm2Js to\nefﬁciently capture the trade-off between them and to fairly\ncompare the different Neuromorphic architectures. Digital\nneuromorphic architectures have made considerable progress\nin recent years, however, little focus has been given to opti-\nmizing ET efﬁciency.\nBased on our analysis of energy consumption, silicon\narea usage and throughput of the state-of-the-art all-digital\nneuromorphic processors [3]–[7], we identify the following\nchallenges that needs to be tackled for achieving the highest\nET efﬁciency: (1) The synapse memory, which holds the\nindividual states and parameters of the synapses is typically\nvery large and contributes signiﬁcantly to the overall energy\nconsumption (and area usage) as can be seen in Figure 1,\nwhich shows the energy consumption breakdown in different\ncomponents of the processor.", "source": "Thor.pdf", "page": 1}
{"chunk_id": "20625a2c-a300-421d-8d95-b22734e15ee7", "text": "Based on our analysis of energy consumption, silicon\narea usage and throughput of the state-of-the-art all-digital\nneuromorphic processors [3]–[7], we identify the following\nchallenges that needs to be tackled for achieving the highest\nET efﬁciency: (1) The synapse memory, which holds the\nindividual states and parameters of the synapses is typically\nvery large and contributes signiﬁcantly to the overall energy\nconsumption (and area usage) as can be seen in Figure 1,\nwhich shows the energy consumption breakdown in different\ncomponents of the processor.\nFigure 1: Energy breakdown of a state-of-the-art neuromorphic\nprocessor [3] running MNIST for a single neuron event (using\ntechnology library 28nm FDSOI at 0.9V@100MHz). Different\ncomponents of the architecture are explained in section III.\nSRAMs are typically used as on-chip memory in most of\nthe all-digital neuromorphic architectures. However, SRAMs\ncomes with different conﬁgurations in terms of number of\nbanks, IO or word width, depth, internal multiplexing fac-\ntor etc, requiring an extensive design space exploration. In\naddition, Standard Cell Memories (SCMs) [8] are becoming\nincreasingly popular as a substitute to relatively smaller sized\nSRAMs due to high energy efﬁciency despite the poor area\nefﬁciency. Optimizing the energy consumption and area usage\nof synapse memory requires an extensive analysis of memory\nhierarchy for the synapse memory including different memory\narchitectures and types. (2) The neuron and synapse memories\nare idle between successive accesses, which contributes to a\nsigniﬁcant amount of idle energy consumption and wastage of\nexpensive on-chip memory bandwidth. This requires a novel\narchitecture with efﬁcient time multiplexing and pipelining of\noperations. (3) The scheduler is designed with ﬁxed number\nof neurons to be processed in parallel, which is a limiting\nfactor for scaling up the architecture for increased throughput.\nTo address these limitations in the state-of-the-art neuromor-\nphic processing architectures and to achieve the highest ET\nefﬁciency, this paper contributes the following:\n1) A neuromorphic processor THOR with a novel archi-\ntecture for neuron update including a parallel neuron\nupdate scheme in the neuron event, a multi-threaded\nscheduler for an increased throughput, and a detailed\nanalysis on the impact of parallelism on the energy\nconsumption. (section IV)\n2) A detailed analysis of the memory hierarchy using\nmultiple memory types and conﬁgurations. Based on our\nanalysis we present the memory selection for THORarXiv:2212.01696v1  [cs.NE]  3 Dec 2022", "source": "Thor.pdf", "page": 1}
{"chunk_id": "9018ca18-9b9e-410a-aafd-1c0ecb44ad54", "text": "with conﬁguration options of the different parameters\n(number of banks, IO or word width, depth, internal\nmultiplexing factor etc). (section V)\n3) We perform post-layout implementation of THOR in\n28nm FDSOI CMOS technology and show a high ET ef-\nﬁciency of 7.29 GSOP2/mm2Js at 0.9V , 400 MHz). (sec-\ntion VI)\nWe review state-of-the-art architectures in section II and\nrelevant background information in section III. In section IV,\nwe present THOR’s architecture followed by an energy ex-\nploration of different design choices and memory hierarchy\nin section V where we make our design choices. Finally, we\npresent our implementation results in section VI and make\na comparison with state-of-the-art architectures and conclude\nour paper with section VII.\nII. R ELATED WORK\nA variety of all-digital neuromorphic compute architectures\nhave been proposed in the past. Truenorth [9], Loihi [10]\nand Spinnaker [11] represent very large scale neuromorphic\narchitectures with multiple cores aimed at ﬂexibility or pro-\ngrammability. Several multi-core architectures [4]–[7], achieve\nlow energy consumption. For example, [7] relies on asyn-\nchronous circuits [12] to wake memories and logic while [4]\nuses routing circuits that rely on spike-driven communication\nto keep energy consumption to a minimum. FPGA based\nneuromorphic architectures [13]–[15], have been proposed\nas well to allow re-conﬁgurability. These architectures pri-\noritize ﬂexibility and programability over energy efﬁciency.\nArchitectures designed for low power embedded applications\n[3]–[5], [7], [16], [17] achieve low energy and area usage,\nbut run with a slow clock, achieving low throughput. For\nexample, in [17] event driven processing with asynchronous\ncomponents acts as a bottleneck. While [18] implements a\ntime multiplexed neuron ALU with event driven clock and\npower gating to achieve high energy efﬁciency on an always-\non architecture. [5] proposes an always-on architecture with\nevent driven clock gating and Globally Asynchronous Locally\nSynchronous (GALS) architecture which achieves 2.1pJ/SOP\nat 0.5V .ODIN [3] uses high density SRAM memory along\nwith a time multiplexed neuron ALU but suffers from poor\nET efﬁciency. To summarize, state-of-the-art neuromorphic\narchitectures with high ﬂexibility have low energy and area\nefﬁciency while energy efﬁcient architectures suffer from low\nthroughput and/or area efﬁciency. In this paper, we aim to\nmaximize ET efﬁciency of digital neuromorphic architecture\nby performing an extensive analysis of energy, area and\nthroughput bottlenecks.\nIII. B ACKGROUND\nThis section ﬁrst introduces the Spiking Neural Networks\n(SNN) that is the class of artiﬁcial neural network supported\nby THOR and then the baseline hardware architecture of a\ndigital neuromorphic processor.\nA. Spiking Neural Networks\nSpiking Neural Networks (SNN) are a class of deep learning\nmodels which attempt to mimic biological nervous systems.SNNs provide several advantages over traditional Artiﬁcial\nNeural Networks (ANN).", "source": "Thor.pdf", "page": 2}
{"chunk_id": "2572a3db-fabb-4d83-b6cc-c11dba7d4fea", "text": "In this paper, we aim to\nmaximize ET efﬁciency of digital neuromorphic architecture\nby performing an extensive analysis of energy, area and\nthroughput bottlenecks.\nIII. B ACKGROUND\nThis section ﬁrst introduces the Spiking Neural Networks\n(SNN) that is the class of artiﬁcial neural network supported\nby THOR and then the baseline hardware architecture of a\ndigital neuromorphic processor.\nA. Spiking Neural Networks\nSpiking Neural Networks (SNN) are a class of deep learning\nmodels which attempt to mimic biological nervous systems.SNNs provide several advantages over traditional Artiﬁcial\nNeural Networks (ANN). The event driven nature of these\nnetworks encourages sparse computation which contributes to\nlow power consumption. Since these networks are based on\nbiological models, they are suitable candidates for biologically\ninspired online learning. The Leaky Integrate-and-ﬁre (LIF)\nneuron, as shown in Figure 2, is a commonly used neuron\nmodel. When a spike arrives on a synapse, it triggers an input\ncurrent into the post-synaptic neuron, which is integrated as\nvoltage called a Synaptic Operation (SOP). The voltage of\nthe neuron leaks according to a time constant. If the neuron\nreaches a certain threshold voltage, it ﬁres an output spike and\nresets its voltage to a resting state.\nFigure 2: Leaky Integrate and ﬁre (LIF) neuron model. Each\nspike arrives at a particular synapse with a corresponding\nsynaptic weight, which increases the membrane potential.\nWhen the membrane potential reaches a certain level, it spikes\nand then returns to initial value.\nB. ODIN Baseline Architecture\nWe selected the state-of-the-art digital architecture ODIN\npresented in [3] as our baseline architecture, that consists of\nan LIF neuron core, synapse core, scheduler and peripher-\nals as shown in Figure 3. We reduce the original baseline\nby removing support for Izhikevich neurons [19]. On-chip\nmemories are used to store the individual states of neurons\nand synapses in the neuron and synapse cores, respectively. A\nScheduler manages the neuron and synaptic updates. Each of\nthe 256 neurons has a fan-in of 256 online-learning synapses,\nto emulated a fully connected 256x256 crossbar. An Address-\nevent representation (AER) interface handles input and output\nevents off-chip [20]. ODIN implements online learning with\nSpike-driven synaptic plasticity (SDSP) [21] and two opera-\ntions: Synapse event : Triggers one speciﬁc synaptic operation\n(SOP) and Neuron event : updates all 256 neurons with 256\nSOPs, according to a source neuron id, by time-multiplexing,\nas shown in Figure 4. Each event takes 2 cycles per SOP and\ninﬂuences online learning. Each word in the synapse memory\ncontains 8 synapses. Hence, synapse memory access takes\nplace every 8 SOPs.\nIV. THOR A RCHITECTURE\nIn this section, we ﬁrst present the top-level THOR archi-\ntecture and then the three main building blocks: neuron core,\nsynapse core and multi-threaded spike scheduler with novel\nimprovements.", "source": "Thor.pdf", "page": 2}
{"chunk_id": "7faa202c-a4dc-41da-a362-d44b2fc61fe2", "text": "Figure 3: Baseline ODIN Architecture after removing the logic\nfor supporting Izhikevich neuron model consists of neuron\ncore, synapse core and spike scheduler.\nFigure 4: Neuron event in ODIN showing the access patterns\nof neuron and synapse SRAMs [3]\nA. Top level architecture\nFigure 5 shows the top level architecture of THOR. The\nmain components are the neuron core, synapse core, and\nmulti-threaded scheduler that consists of dedicated schedulers\nfor output and input spikes. THOR implements an all-to-all\nN neuron network structure similar to the baseline ODIN\narchitecture with the same SDSP online learning rules. Neuron\nand synapse memories are accessed and conﬁgured externally\nthrough the SPI interface. Input events are handled by an AER\ninput block which is part of the controller. The input and\noutput spike schedulers handle propagation of spikes on-chip\nand off-chip respectively.\nFigure 5: THOR - Top level Architecture. Detailed Scheduler\narchitecture in Figure 7As the neuron event is a fundamental operation in SNNs, we\npropose an architecture to achieve a high-throughput neuron\nevent for higher area and throughput efﬁciency. To achieve\nthe high-throughput neuron event, THOR contains two banks\nof neuron and synapse memory where each bank supports\nP-wide memory reads and writes. It also contains P parallel\nneuron and synapse logic units. Read and write operations are\ninterleaved between the two banks to achieve high utilization\nof logic units. Figure 6 shows the timing diagram of our\nmodiﬁed neuron event where P SOPs and memory updates are\nexecuted in parallel in a pipeline. While each P SOPs take two\nclock cycles, we leverage our two-bank memory architecture\nto achieve P operation every cycle with interleaved memory\naccess. During our modiﬁed neuron event , all memories and\nlogic units are fully utilized, resulting in high throughput and\nlow leakage.\n12/3/22, 10:10 PM MOD_NEURON_EVENT .svg\nfile:///C:/Users/20194088/Downloads/MOD_NEURON_EVENT .svg 1/1\nFigure 6: Neuron event timing diagram of THOR processing\n256 SOPs in 9 clock cycles. We have selected the conﬁguraion\nof 32 neurons updates in parallel based on our analysis\npresented in section V.\nB. Neuron Core\nOur neuron core consists of two interleaved neuron memory\nbanks and P LIF neuron logic units. The state of an LIF neuron\nis stored in 7 bytes as shown in Table I. The LIF neuron\nlogic consists of (1) a state update block for integration and\nﬁring. (2) a calcium update block to implement SDSP learning.\nEach memory bank is implemented using 7 sub-banks, one for\neach byte of neuron state (Table I). Each sub-bank has word\nsize of P bytes and consists of N /2P entries. The leakage and\nthreshold memory write circuits are gated during inference,\nas they are only conﬁgured during initialization. Furthermore,\nthe calcium information banks can be gated and disabled if\nonline learning is not being used.", "source": "Thor.pdf", "page": 3}
{"chunk_id": "8487343d-c16a-4ea7-91af-eb1d2d97a01d", "text": "The state of an LIF neuron\nis stored in 7 bytes as shown in Table I. The LIF neuron\nlogic consists of (1) a state update block for integration and\nﬁring. (2) a calcium update block to implement SDSP learning.\nEach memory bank is implemented using 7 sub-banks, one for\neach byte of neuron state (Table I). Each sub-bank has word\nsize of P bytes and consists of N /2P entries. The leakage and\nthreshold memory write circuits are gated during inference,\nas they are only conﬁgured during initialization. Furthermore,\nthe calcium information banks can be gated and disabled if\nonline learning is not being used.\nByte InformationStatus -\nOnline learning disabledStatus -\nOnline learning enabled\n0 Membrane Potential Read/Write Read/Write\n1 Leakage Read-only Read-only\n2 Threshold Read-only Read-only\n3-6 Calcium information UnusedByte 4 - Read/Write\nByte 3,5,6 - Read-only\nTable I: Neuron state breakdown (more details in [3]); 7\nbytes/neuron.\nC. Synapse Core\nThe synapse core consists of logic units and memory\nbanks. The logic blocks are responsible for updating trig-\ngered synapses. Updates are a function of the post-synaptic", "source": "Thor.pdf", "page": 3}
{"chunk_id": "25ed0692-c0b9-46de-99d0-defc0fb24b0c", "text": "membrane potential and calcium information. We implement P\nparallel neuron processing units in the neuron core for parallel\nprocessing.\nD. Multi-threaded Scheduler\nWe implement a multi-threaded scheduler consisting of two\nparallel independent hardware schedulers to handle internal\nand outgoing spikes. The architecture of both schedulers is\nidentical and shown in Figure 7 (a). They consist of a FIFO,\nand a controller which executes an FSM independent of the\nmain controller. The schedulers receive a P-bit spike vector\nfrom the neuron core each cycle during a neuron event,\ndenoting which neurons have spiked, and an offset ( log2N\nbits) to indicate neuron starting address. Whenever a spike is\ndetected in the input, the spike vector and offset are pushed\ninto the FIFO.\n(a)\n (b)\nFigure 7: (a) Multi-threaded scheduler block diagram. The\nFIFO output and \"Send next input\" signals are connected to\nthe AER output interface and to the controller for the output\nand input schedulers respectively.(b) Multi-threaded scheduler\nﬁnite state machine (FSM).\nThe FSM used by the controller is shown in Figure 7 (b).\nWhile the FIFO is not empty, the scheduler decodes an output\nspike from the ﬁrst spike vector in the queue. Output spikes\nare only triggered by a signal \"Send next input\" from the\ncontroller or the AER output, for input and output schedulers\nrespectively, to control trafﬁc. A dedicated status register\ntracks when a spike vector has been exhausted to pop it from\nthe queue. The FIFO has a depth of N /P entries, which is\nsufﬁcient to handle all scenarios. Furthermore, having parallel\nschedulers operating independent results in higher throughput\nas it prevents pipeline stalls.\nV. M EMORY HIERARCHY OPTIMIZATION\nWe consider two memory types: Static Random Access\nMemory (SRAM) and Standard Cell Memory (SCM). SRAMs\nare a common choice for on chip memories and are primarily\ndeﬁned in terms of word size, number of words and mux\nfactor. SCMs consist of arrays of latches or ﬂip-ﬂops and a\nreadout circuit which can be built out of multiplexers, gates or\ntristate buffers [8]. In this section, we explore the efﬁciency\nof different memory hierarchies with different parallelism\nschemes. We ﬁrst compare SRAMs and SCMs for different\nbank sizes and then for different frequencies and degrees of\nparallelism. We conclude the section with our design choices\nbased on analysis results.A. SCM vs SRAMs\nPrior studies have shown that larger SRAMs are area\nefﬁcient that SCMs, however, the latter offers better area\nefﬁciency for smaller sizes. Moreover, the energy consumption\nof SRAM and SCMs have not yet compared in detail for\ndifferent bank sizes, multiplexing factor and I/O width. We\ncompare the energy efﬁciency of SCMs and SRAMs for 32-bit\nand 64-bit wide word sizes and different bank sizes as shown\nin Figure 8. Our analysis shows that SRAM favors larger\nmemory sizes while SCMs do not scale well with memory size\ndue to the increasing overhead logic which increases leakage\npower.", "source": "Thor.pdf", "page": 4}
{"chunk_id": "964a0c8e-b6dc-4bdc-a7e9-a6d0b200d702", "text": "Moreover, the energy consumption\nof SRAM and SCMs have not yet compared in detail for\ndifferent bank sizes, multiplexing factor and I/O width. We\ncompare the energy efﬁciency of SCMs and SRAMs for 32-bit\nand 64-bit wide word sizes and different bank sizes as shown\nin Figure 8. Our analysis shows that SRAM favors larger\nmemory sizes while SCMs do not scale well with memory size\ndue to the increasing overhead logic which increases leakage\npower. SCM performance improves for larger word sizes for\na ﬁxed memory size as having less entries in SCM memory\nreduces the complexity of the decoder and multiplexer logic.\nThe results from Figure 8 show that for speciﬁc memory\nand word size combinations, SCMs can have better energy\nefﬁciency than SRAMs. Furthermore, the use of low leakage\nlibraries and optimal bank sizes improved the energy efﬁciency\nof SCMs. However, SCMs have one order-of-magnitude lower\nenergy efﬁciency compared to SRAMs for large memory sizes.\nFigure 8: Energy analysis of SCM and SRAM for different\nword sizes and different memory sizes (0.9V , 100MHz) in\n28nm FDSOI technology. SCM beneﬁts from smaller memory\nsizes because the periphery circuitry is less complex and power\nhungry than that of an SRAM.\nB. Synapse memory hierarchy\nTo explore different memory hierarchies with different bank\nsizes and degree of parallelism, we deﬁne our synapse memory\nstructure in a generalized manner. Let N be the number of\nneurons, S be the memory bank size in bits, and P be the\ndegree of neuron update parallelism in our design. The synapse\nmemory consists of N2synapses arranged in a crossbar\narchitecture. The total size of the synapse memory is 4N2\nbits and the I/O word width is 4P bits, as we store synapses\nin 4 bits (weights). Figure 9 shows the general architecture\nof the synapse memory. For a given bank size (S), both\nmemory types consist of 4N2/Sbank rows. However, as our\nanalysis was limited by 32-bit word size SRAM macros, we\nhave to partition each SRAM row into 32-bit wide banks\n(4P/32banks), while for SCMs we only have 1 bank per row\nas we could adjust the SCM word size freely. Deﬁning the\nmemory hierarchy in a generalized manner allow us to create\na parameterized synapse memory with SRAM macros of a\nspeciﬁc size. The external address decoders and readout circuit", "source": "Thor.pdf", "page": 4}
{"chunk_id": "5c01256d-e8a9-4d8e-990c-2f85d81508a6", "text": "are synthesized with standard cells to estimate the energy\nand area overhead for multiplexing. We compare SCMs and\nSRAMs for our energy analysis in subsection V-A.\nFigure 9: Synapse Memory Architecture. N = Number of\nneurons, P = Degree of parallelism, S = Bank size in bits\nC. Parallelism exploration\nIn this section, we analyze the impact of memory choices\nwith respect to different parallel designs for THOR. We im-\nplement designs with different amount of parallel neurons and\ndifferent operating frequencies. The designs are implemented\nin 28nm technology with 0.9V target voltage, and synthesized\nwith Cadence Genus.\nThe power numbers are drawn from post-synthesis reports\nand include the overhead of AER handshakes and controller\nstate change. The Energy per synaptic operation Esopis\nconsidered as a metric of energy efﬁciency. We consider\nthe scenario where the chip is saturated with neuron events.\nWe calculate the Energy per neuron event using equation\nESOP = (Ncycles×Tcycle×Pavg)/N, whereNcycles is\nthe number of cycles for a single neuron event, Nis the\nnumber of neurons, and Pavgis the average power. Figure 10\nshows theESOP of different designs with different degrees of\nparallelism, different synapse memory technologies (synapse\nmemory size corresponding to 64K synapses), and operating\nat different frequencies. At lower frequencies, SCM memories\nsuffer from high leakage power. This can be improved using\nlow leakage libraries. Based on the results from Figure 10, we\nchose SCM memories with 32 paralell neuron updates for our\nimplementation as it provides the highest energy efﬁciency.\nNote that although the energy efﬁciency of SRAM is very\nclose to the SCM and yet provides a better area efﬁciency, we\nselected SCM in the ﬁnal implementation to support V oltage-\nFrequency Scaling (which will be limited by SRAMs) in our\nfuture work.\n(a)\n (b)\nFigure 10: Esopfor THOR with SRAM (a) and SCM (b)\n(0.9V) shows that SCM based synapse memories suffer from\nhigh leakage power due to the large area of synapse memory.\nVI. R ESULTS AND COMPARISON\nWe implemented THOR with 32 physical neurons in 28nm\nFDSOI technology (Figure 11) at 400 MHz and 0.9V , with\n4KB and 64KB for the neuron and synapse memories, respec-\ntively. The post-layout netlist energy breakdown is reported in\nFigure 12. All blocks have input and clock gating to reduce\nidle dynamic energy.\nFigure 11: THOR post-synthesis layout. Chip area ( 870µm×\n870µm) is mostly occupied by the SCM synapse memories.", "source": "Thor.pdf", "page": 5}
{"chunk_id": "80aa8d6e-b384-4b82-b021-26f2dbb742e6", "text": "Figure 12: Post-layout Energy Breakdown of a neuron event\n(0.9V , 400MHz).\nAs Figure 12 shows, the memories make signiﬁcant con-\ntributions to THOR’s energy consumption. We use multiple\nthreshold voltage libraries to reduce leakage consumption. For\na P-degree of parallelism, the multiplexing circuit of neuron\nmemory must handle 7P bytes of neuron information. One\ncan improve the energy efﬁciency further with the use of low\nleakage libraries and voltage-frequency scaling, especially for\nthe synapse memories.\nA summary of THOR’s performance compared to state-of-\nthe-art all-digital designs is shown in Table II. µBrain outper-\nforms others in Esopand Energy-Area efﬁciency, as it uses\nasynchronous design techniques and operates at extreme low\nclock frequency (in the Hz range) to reduce the dynamic power\nconsumption. However, THOR has comparable Esopeven with\nthe use of synchronous logic. Moreover, THOR does uses\nSCM instead of SRAM to allow voltage-frequency scaling for\nimproved energy performance. The massive parallel multi-core\narchitectures of [6] and [4] achieve high throughput, however,\nthey have low energy efﬁciency. By combining all the metrics\nEnergy consumption per SOP, Area and Throughput together,\nTHOR outperforms state-of-the-art all-digital neuromorphic\narchitectures by at least 3X.\nVII. C ONCLUSIONS\nWe presented THOR, an all-digital neuromorphic processor\nwith novel architecture for neuron update including a parallel\nneuron update scheme in the neuron event and a multi-\nthreaded scheduler that solves the energy and throughput\nbottlenecks in state-of-the-art processors. We performed an\nenergy analysis of different memory types and conﬁgurations\nand devised the optimal memory hierarchy for neuron and\nsynapse memory. We implemented THOR in 28nm FDSOI\nCMOS technology and demonstrated a single core THOR\nwith an area of 0.77 mm2and an Energy-Area-Throughput\nefﬁciency of 7.37 GSOP2/mm2Js at 0.9V and 400 MHz, with\na 3X improvement of ET efﬁciency compared to state-of-the-\nart digital neuromorphic processors.\nREFERENCES\n[1] S. Narayanan et al. , “Spinalﬂow: An architecture and dataﬂow tailored\nfor spiking neural networks,” in 2020 ACM/IEEE 47th Annual Interna-\ntional Symposium on Computer Architecture (ISCA) , 2020, pp. 349–362.\n[2] M. Pfeiffer and T. Pfeil, “Deep learning with spiking neurons: Oppor-\ntunities and challenges,” Frontiers in Neuroscience , vol. 12, 2018.\n[3] C. Frenkel et al. , “A 0.086-mm, 12.7-pj/sop 64k-synapse 256-neuron\nonline-learning digital spiking neuromorphic processor in 28-nm cmos,”\nIEEE transactions on biomedical circuits and systems , vol. 13, no. 1,\npp. 145–158, 2018.[4] Y . Kuang et al.", "source": "Thor.pdf", "page": 6}
{"chunk_id": "2837c508-e53e-49b7-9325-74ae1b411c53", "text": "349–362.\n[2] M. Pfeiffer and T. Pfeil, “Deep learning with spiking neurons: Oppor-\ntunities and challenges,” Frontiers in Neuroscience , vol. 12, 2018.\n[3] C. Frenkel et al. , “A 0.086-mm, 12.7-pj/sop 64k-synapse 256-neuron\nonline-learning digital spiking neuromorphic processor in 28-nm cmos,”\nIEEE transactions on biomedical circuits and systems , vol. 13, no. 1,\npp. 145–158, 2018.[4] Y . Kuang et al. , “A 64k-neuron 64m-1b-synapse 2.64 pj/sop neuromor-\nphic chip with all memory on chip for spike-based models in 65nm\ncmos,” IEEE Transactions on Circuits and Systems II: Express Briefs ,\nvol. 68, no. 7, pp. 2655–2659, 2021.\n[5] M. Wong et al. , “A 2.1 pj/sop 40nm snn accelerator featuring on-\nchip transfer learning using delta stdp,” in ESSDERC 2021-IEEE 51st\nEuropean Solid-State Device Research Conference , 2021, pp. 95–98.\n[6] G. K. Chen et al. , “A 4096-neuron 1m-synapse 3.8-pj/sop spiking neural\nnetwork with on-chip stdp learning and sparse weights in 10-nm ﬁnfet\ncmos,” IEEE JSSC , vol. 54, no. 4, pp. 992–1002, 2018.\n[7] J. Zhang et al. , “A 28nm conﬁgurable asynchronous snn accelerator with\nenergy-efﬁcient learning,” in 2021 27th IEEE International Symposium\non Asynchronous Circuits and Systems . IEEE, 2021, pp. 34–39.\n[8] A. Teman et al. , “Power, area, and performance optimization of standard\ncell memory arrays through controlled placement,” ACM Transactions\non Design Automation of Electronic Systems (TODAES) , vol. 21, no. 4,\npp. 1–25, 2016.\n[9] F. Akopyan et al. , “Truenorth: Design and tool ﬂow of a 65 mw 1\nmillion neuron programmable neurosynaptic chip,” IEEE transactions\non computer-aided design of integrated circuits and systems , vol. 34,\nno. 10, pp. 1537–1557, 2015.\n[10] M. Davies et al. , “Loihi: A neuromorphic manycore processor with on-\nchip learning,” Ieee Micro , vol. 38, no. 1, pp. 82–99, 2018.\n[11] E. Painkras et al. , “Spinnaker: A multi-core system-on-chip for\nmassively-parallel neural net simulation,” in Proceedings of the IEEE\n2012 Custom Integrated Circuits Conference . IEEE, 2012, pp. 1–4.\n[12] A. Peeters et al. , “Click elements: An implementation style for data-\ndriven compilation,” in 2010 IEEE Symposium on Asynchronous Circuits\nand Systems .", "source": "Thor.pdf", "page": 6}
{"chunk_id": "cfc5d919-35f2-42a5-b57d-d2338c009a32", "text": ", “Loihi: A neuromorphic manycore processor with on-\nchip learning,” Ieee Micro , vol. 38, no. 1, pp. 82–99, 2018.\n[11] E. Painkras et al. , “Spinnaker: A multi-core system-on-chip for\nmassively-parallel neural net simulation,” in Proceedings of the IEEE\n2012 Custom Integrated Circuits Conference . IEEE, 2012, pp. 1–4.\n[12] A. Peeters et al. , “Click elements: An implementation style for data-\ndriven compilation,” in 2010 IEEE Symposium on Asynchronous Circuits\nand Systems . IEEE, 2010, pp. 3–14.\n[13] X. Huang et al. , “Spiking neural network based low-power radioisotope\nidentiﬁcation using fpga,” in 2020 27th IEEE International Conference\non Electronics, Circuits and Systems (ICECS) . IEEE, 2020, pp. 1–4.\n[14] J. P. Mitchell et al. , “A small, low cost event-driven architecture for\nspiking neural networks on fpgas,” in International Conference on\nNeuromorphic Systems 2020 , 2020, pp. 1–4.\n[15] H. Irmak et al. , “A dynamic reconﬁgurable architecture for hybrid\nspiking and convolutional fpga-based neural network designs,” Journal\nof Low Power Electronics and Applications , vol. 11, no. 3, p. 32, 2021.\n[16] H. Lee et al. , “Neuroengine: a hardware-based event-driven simulation\nsystem for advanced brain-inspired computing,” in Proceedings of\nthe 26th ACM International Conference on Architectural Support for\nProgramming Languages and Operating Systems , 2021, pp. 975–989.\n[17] J. Stuijt et al. , “µbrain: An event-driven and fully synthesizable archi-\ntecture for spiking neural networks,” Frontiers in neuroscience , vol. 15,\np. 538, 2021.\n[18] D. Wang et al. , “Always-on, sub-300-nw, event-driven spiking neural\nnetwork based on spike-driven clock-generation and clock-and power-\ngating for an ultra-low-power intelligent device,” in 2020 IEEE Asian\nSolid-State Circuits Conference (A-SSCC) . IEEE, 2020, pp. 1–4.\n[19] C. Frenkel, J.-D. Legat, and D. Bol, “A compact phenomenological\ndigital neuron implementing the 20 izhikevich behaviors,” in 2017 IEEE\nBiomedical Circuits and Systems Conference (BioCAS) , 2017, pp. 1–4.\n[20] K. Boahen, “Point-to-point connectivity between neuromorphic chips\nusing address events,” IEEE Transactions on Circuits and Systems II:\nAnalog and Digital Signal Processing , vol. 47, no. 5, pp. 416–434, 2000.\n[21] J. M. Brader et al. , “Learning real-world stimuli in a neural network with\nspike-driven synaptic dynamics,” Neural computation , vol. 19, no. 11,\npp. 2881–2912, 2007.", "source": "Thor.pdf", "page": 6}
{"chunk_id": "18786353-53a3-446d-8c28-67962addcfb9", "text": "mu-brain [17] Wang [18] Kuang [4] Wong [5] Chen [6] Zhang [7] ODIN [3] THOR\nCircuit type Asynchronous Synchronous Synchronous Synchronous Synchronous Asynchronous Synchronous Synchronous\nTechnology (nm) 40 65 65 40 10 28 28 28\nTotal area (mm2) 1.42 1.99 89.48 14.57 1.28 0.52 0.086 0.77\nNumber of cores 1 1 64 44 64 1 1 1\nNeurons 336 650 64K 11K 64K 256 256 256\nSynapses 37K 67K 64M-total 2.8M 64M 131K 65K 65K\nOnline learning No No No Yes Yes Yes Yes Yes\nEnergy per SOP (pJ/SOP)0.627\n@1.1V1.5\n@70Khz,0.5V2.64\n@ 24MHz,0.89V\n4.60\n@192MHz,1.20V2.1\n@12.5MHz,0.5V\n9.5\n@160MHz,1V3.8\n@ 105MHz,0.52 V\n8.3\n@ 506MHz,0.9V3.97\n@6.7MHz,0.8V8.40\n@75MHz,0.55V1.40\n@400MHz,0.9V\nThroughput (SOP/s)16 K\n@ 1.4Hz\ninput rate260 K\n@ 0.5V ,70Khz12.29G\n@ 192MHz,1,20V0.28G\n@12.5MHz,0.5V\n1.93G\n@160MHz,1V5.16G\n@ 105MHz,0.52V\n25.11G\n@ 506MHz,0.9V0.01G\n@6.7MHz,0.8V37.5M\n@ 75MHz,0.55V7.84G\n@ 400MHz,0.9V\nEnergy-Throughput (ET)\n(TSOP2/mm2Js)17.92K\n@1.4hz\ninput rate88.4K\n@0.5V ,70Khz0.049G\n@ 192MHz,1,20V0.009G\n@12.5MHz,0.5V\n0.014G\n@160MHz,1V1.03G\n@ 105MHz,0.52V\n2.25G\n@ 506MHz,0.9V0.005G\n@6.7MHz,0.8V51.9M\n@75MHz,0.55V7.29G\n@400MHz,0.9V\nTable II: Comparison of THOR with state-of-the-art neuromorphic processors. THOR outperforms the state-of-the-art\narchitectures by a factor of over 3X in terms of ET efﬁciency.", "source": "Thor.pdf", "page": 7}
